{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a49ed07b",
   "metadata": {},
   "source": [
    "# Q1: In this question you have to scrape data using the filters available on the webpage You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the web page https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59742464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape data from naukri.com\n",
    "def scrape_naukri_data():\n",
    "    # URL of the website\n",
    "    url = 'https://www.naukri.com/'\n",
    "\n",
    "    # Requesting the page\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Checking if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        print(\"Connection successful!\")\n",
    "    else:\n",
    "        print(\"Failed to connect to the website!\")\n",
    "        return None\n",
    "    \n",
    "    # Parsing HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Finding the search form and filling it with required details\n",
    "    search_form = soup.find('form', attrs={'id': 'quicksearch'})\n",
    "    search_field = search_form.find('input', attrs={'id': 'qsb-keyword-sugg'})\n",
    "    search_field['value'] = 'Data Scientist'\n",
    "    \n",
    "    # Submitting the form\n",
    "    response = requests.post(url, data=search_form)\n",
    "    \n",
    "    # Checking if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        print(\"Search successful!\")\n",
    "    else:\n",
    "        print(\"Failed to submit the search form!\")\n",
    "        return None\n",
    "    \n",
    "    # Applying filters\n",
    "    filters = {\n",
    "        'locations': 'Delhi/NCR',\n",
    "        'ctcFilter': '3-6'\n",
    "    }\n",
    "    for filter_name, value in filters.items():\n",
    "        filter_input = search_form.find('input', attrs={'name': filter_name})\n",
    "        if filter_input:\n",
    "            filter_input['value'] = value\n",
    "    \n",
    "    # Submitting the filtered form\n",
    "    response = requests.post(url, data=search_form)\n",
    "    \n",
    "    # Checking if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        print(\"Filters applied successfully!\")\n",
    "    else:\n",
    "        print(\"Failed to apply filters!\")\n",
    "        return None\n",
    "    \n",
    "    # Parsing the filtered page\n",
    "    filtered_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Scraping job details\n",
    "    jobs = []\n",
    "    job_cards = filtered_soup.find_all('article', class_='jobTuple')\n",
    "    for job_card in job_cards[:10]:\n",
    "        title = job_card.find('a', class_='title').text.strip()\n",
    "        location = job_card.find('li', class_='location').text.strip()\n",
    "        company = job_card.find('a', class_='subTitle').text.strip()\n",
    "        experience = job_card.find('li', class_='experience').text.strip()\n",
    "        jobs.append({\n",
    "            'Title': title,\n",
    "            'Location': location,\n",
    "            'Company': company,\n",
    "            'Experience': experience\n",
    "        })\n",
    "    \n",
    "    return jobs\n",
    "\n",
    "# Main function to execute the scraping and create dataframe\n",
    "def main():\n",
    "    data = scrape_naukri_data()\n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        print(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8f0db6",
   "metadata": {},
   "source": [
    "# 02Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "#This task will be done in following steps:\n",
    "1. First get the webpage https://www.shine.com/\n",
    "2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the searchbutton.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dc7dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_shine_data():\n",
    "    url = 'https://www.shine.com/'\n",
    "    response = requests.post(url, data={'q': 'Data Scientist', 'l': 'Bangalore'})\n",
    "    if response.status_code != 200: return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    jobs = []\n",
    "    for job_card in soup.find_all('li', class_='sjsresult')[:10]:\n",
    "        title = job_card.find('h2').text.strip()\n",
    "        location = job_card.find('span', class_='loc').text.strip()\n",
    "        company = job_card.find('span', class_='snp').text.strip()\n",
    "        experience = job_card.find('li', class_='exp').text.strip()\n",
    "        jobs.append({'Job Title': title, 'Job Location': location, 'Company Name': company, 'Experience Required': experience})\n",
    "    return jobs\n",
    "\n",
    "def main():\n",
    "    data = scrape_shine_data()\n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        print(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269c580c",
   "metadata": {},
   "source": [
    "# Q3: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\n",
    "As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decf34bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_flipkart_reviews():\n",
    "    url = 'https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200: return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    reviews = []\n",
    "    \n",
    "    for review_card in soup.find_all('div', class_='col _2wzgFH K0kLPL')[:100]:\n",
    "        rating = review_card.find('div', class_='hGSR34').text.strip()\n",
    "        review_summary = review_card.find('p', class_='_2-N8zT').text.strip()\n",
    "        full_review = review_card.find('div', class_='qwjRop').text.strip()\n",
    "        reviews.append({'Rating': rating, 'Review Summary': review_summary, 'Full Review': full_review})\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "def main():\n",
    "    reviews = scrape_flipkart_reviews()\n",
    "    for idx, review in enumerate(reviews):\n",
    "        print(f\"Review {idx+1}:\")\n",
    "        print(f\"Rating: {review['Rating']}\")\n",
    "        print(f\"Review Summary: {review['Review Summary']}\")\n",
    "        print(f\"Full Review: {review['Full Review']}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47f24f0",
   "metadata": {},
   "source": [
    "# Q4: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.\n",
    "You have to scrape 3 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the above attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f338df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.flipkart.com/search?q=sneakers\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "products = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "\n",
    "sneakers_list = []\n",
    "for product in products:\n",
    "    if len(sneakers_list) >= 100:\n",
    "        break\n",
    "    try:\n",
    "        brand = product.find('div', {'class': '_2WkVRV'}).text\n",
    "        description = product.find('a', {'class': 'IRpwTa'}).text\n",
    "        price = product.find('div', {'class': '_30jeq3'}).text\n",
    "        sneakers_list.append([brand, description, price])\n",
    "    except AttributeError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3660375f",
   "metadata": {},
   "source": [
    "# 07Write a python program to display list of respected former Prime Ministers of India (i.e. Name, Born-Dead, Term of office, Remarks) from https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1\n",
    "scrap the mentioned data and make the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf42ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = \"https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the table containing the data\n",
    "table = soup.find('table')\n",
    "\n",
    "# Lists to store the scraped data\n",
    "data = []\n",
    "\n",
    "# Iterate through the rows of the table\n",
    "for row in table.find_all('tr')[1:]:  # Skip the header row\n",
    "    cols = [col.text.strip() for col in row.find_all('td')]\n",
    "    if len(cols) == 4:\n",
    "        data.append(cols)\n",
    "\n",
    "# Create a DataFrame with the scraped data\n",
    "df = pd.DataFrame(data, columns=['Name', 'Born-Dead', 'Term of Office', 'Remarks'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "df.to_csv('indian_prime_ministers.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de76a8f",
   "metadata": {},
   "source": [
    "# 08 Write a python program to display list of 50 Most expensive cars in the world (i.e. Car name and Price) from https://www.motor1.com/\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.motor1.com/\n",
    "2. Then You have to type in the search bar ’50 most expensive cars’\n",
    "3. Then click on 50 most expensive cars in the world..\n",
    "4. Then scrap the mentioned data and make the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2e83b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = 'https://www.motor1.com/'\n",
    "response = requests.get(url)\n",
    "if response.status_code != 200:\n",
    "    print(\"Failed to fetch the webpage.\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Type in the search bar '50 most expensive cars'\n",
    "search_query = \"50 most expensive cars\"\n",
    "search_url = f\"https://www.motor1.com/?s={search_query.replace(' ', '+')}\"\n",
    "search_response = requests.get(search_url)\n",
    "if search_response.status_code != 200:\n",
    "    print(\"Failed to execute the search query.\")\n",
    "    exit()\n",
    "\n",
    "# Step 3: Click on '50 most expensive cars in the world'\n",
    "soup = BeautifulSoup(search_response.text, 'html.parser')\n",
    "expensive_cars_link = soup.find('a', text='50 Most Expensive Cars In The World')\n",
    "if not expensive_cars_link:\n",
    "    print(\"Failed to find the link for the 50 most expensive cars.\")\n",
    "    exit()\n",
    "\n",
    "expensive_cars_url = expensive_cars_link['href']\n",
    "\n",
    "# Step 4: Scrape the mentioned data and make the dataframe\n",
    "expensive_cars_response = requests.get(expensive_cars_url)\n",
    "if expensive_cars_response.status_code != 200:\n",
    "    print(\"Failed to fetch data for the 50 most expensive cars.\")\n",
    "    exit()\n",
    "\n",
    "expensive_cars_soup = BeautifulSoup(expensive_cars_response.text, 'html.parser')\n",
    "\n",
    "# Extract data\n",
    "car_data = []\n",
    "car_table = expensive_cars_soup.find('table', class_='table-wrapper')\n",
    "if car_table:\n",
    "    rows = car_table.find_all('tr')[1:]  # Skip the header row\n",
    "    for row in rows:\n",
    "        cells = row.find_all('td')\n",
    "        car_name = cells[0].text.strip()\n",
    "        car_price = cells[1].text.strip()\n",
    "        car_data.append([car_name, car_price])\n",
    "\n",
    "# Display data\n",
    "for car in car_data:\n",
    "    print(f\"Car Name: {car[0]}, Price: {car[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e962d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ed91d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d92c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
